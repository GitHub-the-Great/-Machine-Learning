{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwNe4f31_49L",
        "outputId": "bd22eb66-26fc-4a42-f995-927ac9641149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Hidden Layer Outputs (z1) for Each Activation Function ===\n",
            "\n",
            "            Hidden Neuron 1  Hidden Neuron 2  Hidden Neuron 3  Hidden Neuron 4\n",
            "tanh                 0.2165           0.1391           0.1194           0.1489\n",
            "hard_tanh            0.2200           0.1400           0.1200           0.1500\n",
            "softplus             0.8092           0.7656           0.7549           0.7710\n",
            "relu                 0.2200           0.1400           0.1200           0.1500\n",
            "leaky_relu           0.2200           0.1400           0.1200           0.1500\n",
            "\n",
            "=== Example Forward Pass: tanh ===\n",
            "Input x (with bias):\n",
            " [[1.  0.5 0.2 0.1]]\n",
            "Hidden pre-activation a1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden activation z1:\n",
            " [[0.21651806 0.13909245 0.1194273  0.14888503]]\n",
            "Hidden layer with bias z1_aug:\n",
            " [[1.         0.21651806 0.13909245 0.1194273  0.14888503]]\n",
            "Final output y:\n",
            " [[ 0.32564833 -0.05383076]]\n",
            "\n",
            "=== Example Forward Pass: hard_tanh ===\n",
            "Input x (with bias):\n",
            " [[1.  0.5 0.2 0.1]]\n",
            "Hidden pre-activation a1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden activation z1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden layer with bias z1_aug:\n",
            " [[1.   0.22 0.14 0.12 0.15]]\n",
            "Final output y:\n",
            " [[ 0.327 -0.052]]\n",
            "\n",
            "=== Example Forward Pass: softplus ===\n",
            "Input x (with bias):\n",
            " [[1.  0.5 0.2 0.1]]\n",
            "Hidden pre-activation a1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden activation z1:\n",
            " [[0.80918502 0.76559518 0.7549461  0.77095705]]\n",
            "Hidden layer with bias z1_aug:\n",
            " [[1.         0.80918502 0.76559518 0.7549461  0.77095705]]\n",
            "Final output y:\n",
            " [[0.82076474 0.43204936]]\n",
            "\n",
            "=== Example Forward Pass: relu ===\n",
            "Input x (with bias):\n",
            " [[1.  0.5 0.2 0.1]]\n",
            "Hidden pre-activation a1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden activation z1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden layer with bias z1_aug:\n",
            " [[1.   0.22 0.14 0.12 0.15]]\n",
            "Final output y:\n",
            " [[ 0.327 -0.052]]\n",
            "\n",
            "=== Example Forward Pass: leaky_relu ===\n",
            "Input x (with bias):\n",
            " [[1.  0.5 0.2 0.1]]\n",
            "Hidden pre-activation a1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden activation z1:\n",
            " [[0.22 0.14 0.12 0.15]]\n",
            "Hidden layer with bias z1_aug:\n",
            " [[1.   0.22 0.14 0.12 0.15]]\n",
            "Final output y:\n",
            " [[ 0.327 -0.052]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define activation functions\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def hard_tanh(x):\n",
        "    return np.maximum(-1, np.minimum(1, x))\n",
        "\n",
        "def softplus(x):\n",
        "    return np.log1p(np.exp(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, alpha=0.1):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Input vector (3 features + bias)\n",
        "x_raw = np.array([[0.5], [0.2], [0.1]])  # shape: (3,1)\n",
        "x = np.vstack(([1.0], x_raw))           # shape: (4,1)\n",
        "\n",
        "# Weights\n",
        "W1 = np.array([\n",
        "    [0.1, 0.1, 0.2, 0.3],\n",
        "    [0.2, -0.3, 0.4, 0.1],\n",
        "    [0.05, 0.2, -0.2, 0.1],\n",
        "    [0.0, 0.3, -0.1, 0.2]\n",
        "])  # shape: (4,4)\n",
        "\n",
        "W2 = np.array([\n",
        "    [0.2, 0.3, -0.1, 0.5, 0.1],\n",
        "    [-0.2, 0.4, 0.3, -0.1, 0.2]\n",
        "])  # shape: (2,5)\n",
        "\n",
        "# Forward pass function\n",
        "def forward_pass(x, W1, W2, activation_func):\n",
        "    a1 = W1 @ x\n",
        "    z1 = activation_func(a1)\n",
        "    z1_aug = np.vstack(([1.0], z1))  # add bias\n",
        "    y = W2 @ z1_aug\n",
        "    return a1, z1, z1_aug, y\n",
        "\n",
        "# Evaluate all activation functions\n",
        "results = {}\n",
        "activations = {\n",
        "    \"tanh\": tanh,\n",
        "    \"hard_tanh\": hard_tanh,\n",
        "    \"softplus\": softplus,\n",
        "    \"relu\": relu,\n",
        "    \"leaky_relu\": leaky_relu\n",
        "}\n",
        "\n",
        "for name, func in activations.items():\n",
        "    a1, z1, z1_aug, y = forward_pass(x, W1, W2, func)\n",
        "    results[name] = {\n",
        "        \"a1\": a1,\n",
        "        \"z1\": z1,\n",
        "        \"z1_aug\": z1_aug,\n",
        "        \"y\": y\n",
        "    }\n",
        "\n",
        "# Compare hidden layer outputs\n",
        "print(\"=== Hidden Layer Outputs (z1) for Each Activation Function ===\\n\")\n",
        "df = pd.DataFrame({name: result[\"z1\"].flatten() for name, result in results.items()}).T\n",
        "df.columns = [f\"Hidden Neuron {i+1}\" for i in range(df.shape[1])]\n",
        "print(df.round(4))  # Rounded for readability\n",
        "\n",
        "# Print one full example (tanh) for manual checking\n",
        "print(\"\\n=== Example Forward Pass: tanh ===\")\n",
        "print(\"Input x (with bias):\\n\", x.T)\n",
        "print(\"Hidden pre-activation a1:\\n\", results[\"tanh\"][\"a1\"].T)\n",
        "print(\"Hidden activation z1:\\n\", results[\"tanh\"][\"z1\"].T)\n",
        "print(\"Hidden layer with bias z1_aug:\\n\", results[\"tanh\"][\"z1_aug\"].T)\n",
        "print(\"Final output y:\\n\", results[\"tanh\"][\"y\"].T)\n",
        "\n",
        "# Print one full example (hard_tanh) for manual checking\n",
        "print(\"\\n=== Example Forward Pass: hard_tanh ===\")\n",
        "print(\"Input x (with bias):\\n\", x.T)\n",
        "print(\"Hidden pre-activation a1:\\n\", results[\"hard_tanh\"][\"a1\"].T)\n",
        "print(\"Hidden activation z1:\\n\", results[\"hard_tanh\"][\"z1\"].T)\n",
        "print(\"Hidden layer with bias z1_aug:\\n\", results[\"hard_tanh\"][\"z1_aug\"].T)\n",
        "print(\"Final output y:\\n\", results[\"hard_tanh\"][\"y\"].T)\n",
        "\n",
        "# Print one full example (softplus) for manual checking\n",
        "print(\"\\n=== Example Forward Pass: softplus ===\")\n",
        "print(\"Input x (with bias):\\n\", x.T)\n",
        "print(\"Hidden pre-activation a1:\\n\", results[\"softplus\"][\"a1\"].T)\n",
        "print(\"Hidden activation z1:\\n\", results[\"softplus\"][\"z1\"].T)\n",
        "print(\"Hidden layer with bias z1_aug:\\n\", results[\"softplus\"][\"z1_aug\"].T)\n",
        "print(\"Final output y:\\n\", results[\"softplus\"][\"y\"].T)\n",
        "\n",
        "# Print one full example (relu) for manual checking\n",
        "print(\"\\n=== Example Forward Pass: relu ===\")\n",
        "print(\"Input x (with bias):\\n\", x.T)\n",
        "print(\"Hidden pre-activation a1:\\n\", results[\"relu\"][\"a1\"].T)\n",
        "print(\"Hidden activation z1:\\n\", results[\"relu\"][\"z1\"].T)\n",
        "print(\"Hidden layer with bias z1_aug:\\n\", results[\"relu\"][\"z1_aug\"].T)\n",
        "print(\"Final output y:\\n\", results[\"relu\"][\"y\"].T)\n",
        "\n",
        "# Print one full example (leaky_relu) for manual checking\n",
        "print(\"\\n=== Example Forward Pass: leaky_relu ===\")\n",
        "print(\"Input x (with bias):\\n\", x.T)\n",
        "print(\"Hidden pre-activation a1:\\n\", results[\"leaky_relu\"][\"a1\"].T)\n",
        "print(\"Hidden activation z1:\\n\", results[\"leaky_relu\"][\"z1\"].T)\n",
        "print(\"Hidden layer with bias z1_aug:\\n\", results[\"leaky_relu\"][\"z1_aug\"].T)\n",
        "print(\"Final output y:\\n\", results[\"leaky_relu\"][\"y\"].T)\n"
      ]
    }
  ]
}